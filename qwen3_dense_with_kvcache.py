import torch
import json
import math
from safetensors import safe_open
from transformers import AutoTokenizer

def custom_gemm(a, b):
    """è‡ªå®šä¹‰GEMMå®ç°"""
    original_shape = a.shape
    a_flat = a.view(-1, a.size(-1))
    result = torch.mm(a_flat, b)
    return result.view(*original_shape[:-1], b.size(-1))

class WeightManager:
    def __init__(self, safetensors_path):
        self.weights = {}
        with safe_open(safetensors_path, framework="pt", device="cpu") as f:
            for key in f.keys():
                self.weights[key] = f.get_tensor(key).clone().float()
        print(f"åŠ è½½äº† {len(self.weights)} ä¸ªæƒé‡")

    def get_weight(self, key):
        return self.weights.get(key)

class CustomRMSNorm:
    def __init__(self, weight, eps=1e-6):
        self.weight = weight
        self.eps = eps

    def forward(self, x):
        norm = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return norm * self.weight

class CustomLinear:
    def __init__(self, weight, bias=None):
        self.weight = weight
        self.bias = bias

    def forward(self, x):
        output = custom_gemm(x, self.weight.t())
        if self.bias is not None:
            output = output + self.bias
        return output

class CustomEmbedding:
    def __init__(self, weight):
        self.weight = weight

    def forward(self, input_ids):
        return self.weight[input_ids]

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None):
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç """
    if position_ids is None:
        seq_len = q.shape[-2]
        position_ids = torch.arange(seq_len, dtype=torch.long, device=q.device)
        position_ids = position_ids.unsqueeze(0)

    cos = cos[position_ids]
    sin = sin[position_ids]

    def rotate_half(x):
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2 :]
        return torch.cat((-x2, x1), dim=-1)

    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

class QwenRotaryEmbedding:
    def __init__(self, dim, max_position_embeddings=2048, base=10000):
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base

        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))
        self.register_buffer("inv_freq", inv_freq)

        max_seq_len = max_position_embeddings
        t = torch.arange(max_seq_len, dtype=torch.float)
        freqs = torch.outer(t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.cos_cached = emb.cos()
        self.sin_cached = emb.sin()

    def register_buffer(self, name, tensor):
        setattr(self, name, tensor)

    def forward(self, x, seq_len=None):
        if seq_len > self.cos_cached.shape[0]:
            t = torch.arange(seq_len, dtype=torch.float)
            freqs = torch.outer(t, self.inv_freq)
            emb = torch.cat((freqs, freqs), dim=-1)
            self.cos_cached = emb.cos()
            self.sin_cached = emb.sin()

        return (
            self.cos_cached[:seq_len],
            self.sin_cached[:seq_len],
        )

class KVCache:
    """ç®€åŒ–çš„KV Cacheå®ç°"""
    def __init__(self):
        self.cache_k = None
        self.cache_v = None

    def update(self, k, v):
        """æ›´æ–°KVç¼“å­˜"""
        if self.cache_k is None:
            # é¦–æ¬¡ç¼“å­˜
            self.cache_k = k
            self.cache_v = v
        else:
            # æ‹¼æ¥æ–°çš„k,v
            self.cache_k = torch.cat([self.cache_k, k], dim=-2)
            self.cache_v = torch.cat([self.cache_v, v], dim=-2)

        return self.cache_k, self.cache_v

    def reset(self):
        """é‡ç½®ç¼“å­˜"""
        self.cache_k = None
        self.cache_v = None

class Qwen3AttentionWithKVCache:
    """å¸¦KV Cacheçš„Qwen3æ³¨æ„åŠ›å®ç°"""
    def __init__(self, layer_idx, weight_manager, config):
        self.layer_idx = layer_idx
        self.hidden_size = config['hidden_size']
        self.num_heads = config['num_attention_heads']
        self.num_key_value_heads = config['num_key_value_heads']
        self.head_dim = config['head_dim']
        self.max_position_embeddings = config['max_position_embeddings']

        assert self.num_heads % self.num_key_value_heads == 0

        # æŠ•å½±å±‚
        prefix = f"model.layers.{layer_idx}.self_attn"
        self.q_proj = CustomLinear(weight_manager.get_weight(f"{prefix}.q_proj.weight"))
        self.k_proj = CustomLinear(weight_manager.get_weight(f"{prefix}.k_proj.weight"))
        self.v_proj = CustomLinear(weight_manager.get_weight(f"{prefix}.v_proj.weight"))
        self.o_proj = CustomLinear(weight_manager.get_weight(f"{prefix}.o_proj.weight"))

        # RMSNorm for qå’Œk
        self.q_norm = CustomRMSNorm(weight_manager.get_weight(f"{prefix}.q_norm.weight"))
        self.k_norm = CustomRMSNorm(weight_manager.get_weight(f"{prefix}.k_norm.weight"))

        # RoPE
        self.rotary_emb = QwenRotaryEmbedding(
            self.head_dim,
            max_position_embeddings=self.max_position_embeddings
        )

        # KV Cache
        self.kv_cache = KVCache()

    def forward(self, hidden_states, attention_mask=None, position_ids=None, use_cache=False, cache_position=None):
        batch_size, seq_len, _ = hidden_states.shape
        # æŠ•å½±
        q = self.q_proj.forward(hidden_states)
        k = self.k_proj.forward(hidden_states)
        v = self.v_proj.forward(hidden_states)

        # é‡å¡‘
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)

        # åº”ç”¨RMSNorm
        q = self.q_norm.forward(q)
        k = self.k_norm.forward(k)

        # è½¬ç½®ä¸º (batch, heads, seq, head_dim)
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # åº”ç”¨RoPE - éœ€è¦æ ¹æ®å®é™…çš„åºåˆ—ä½ç½®è®¡ç®—
        if position_ids is None:
            if use_cache and self.kv_cache.cache_k is not None:
                # å¢é‡æ¨ç†ï¼šå½“å‰ä½ç½® = å·²ç¼“å­˜çš„é•¿åº¦
                current_pos = self.kv_cache.cache_k.shape[-2]
                position_ids = torch.arange(current_pos, current_pos + seq_len, dtype=torch.long, device=hidden_states.device).unsqueeze(0)
            else:
                # é¦–æ¬¡æ¨ç†ï¼šä»0å¼€å§‹
                position_ids = torch.arange(seq_len, dtype=torch.long, device=hidden_states.device).unsqueeze(0)

        cos, sin = self.rotary_emb.forward(q, seq_len=position_ids.max().item() + 1)
        q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids)

        # ä½¿ç”¨KV Cache
        if use_cache:
            k, v = self.kv_cache.update(k, v)
        #     print("k shape : ", k.shape)
        #     print("v shape : ", v.shape)
        # else:
        #     print("k shape : ", k.shape)
        #     print("v shape : ", v.shape)
        # GQA: æ‰©å±•kå’Œv
        if self.num_key_value_heads != self.num_heads:
            k = k.repeat_interleave(self.num_heads // self.num_key_value_heads, dim=1)
            v = v.repeat_interleave(self.num_heads // self.num_key_value_heads, dim=1)

        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # å¤„ç†attention mask
        if use_cache and self.kv_cache.cache_k is not None and seq_len == 1:
            # å¢é‡æ¨ç†æ—¶ï¼Œqueryåªæœ‰1ä¸ªtokenï¼Œkeyæœ‰å¤šä¸ªtokenï¼Œä¸éœ€è¦æ©ç 
            pass
        elif attention_mask is not None:
            scores = scores + attention_mask

        attn_weights = torch.softmax(scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)

        # è½¬å›åŸå§‹æ ¼å¼
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.num_heads * self.head_dim)
        
        return self.o_proj.forward(attn_output)

class CustomQwen3MLP:
    def __init__(self, layer_idx, weight_manager):
        prefix = f"model.layers.{layer_idx}.mlp"
        self.gate_proj = CustomLinear(weight_manager.get_weight(f"{prefix}.gate_proj.weight"))
        self.up_proj = CustomLinear(weight_manager.get_weight(f"{prefix}.up_proj.weight"))
        self.down_proj = CustomLinear(weight_manager.get_weight(f"{prefix}.down_proj.weight"))

    def forward(self, x):
        gate = torch.nn.functional.silu(self.gate_proj.forward(x))
        up = self.up_proj.forward(x)
        
        return self.down_proj.forward(gate * up)

class Qwen3DecoderLayerWithKVCache:
    def __init__(self, layer_idx, weight_manager, config):
        self.self_attn = Qwen3AttentionWithKVCache(layer_idx, weight_manager, config)
        self.mlp = CustomQwen3MLP(layer_idx, weight_manager)

        prefix = f"model.layers.{layer_idx}"
        self.input_layernorm = CustomRMSNorm(
            weight_manager.get_weight(f"{prefix}.input_layernorm.weight"),
            eps=config['rms_norm_eps']
        )
        self.post_attention_layernorm = CustomRMSNorm(
            weight_manager.get_weight(f"{prefix}.post_attention_layernorm.weight"),
            eps=config['rms_norm_eps']
        )

    def forward(self, hidden_states, attention_mask=None, position_ids=None, use_cache=False, cache_position=None):
        # Self-attention
        residual = hidden_states
        hidden_states = self.input_layernorm.forward(hidden_states)
        hidden_states = self.self_attn.forward(
            hidden_states, attention_mask, position_ids, use_cache, cache_position
        )
        hidden_states = residual + hidden_states

        # MLP
        residual = hidden_states
        hidden_states = self.post_attention_layernorm.forward(hidden_states)
        hidden_states = self.mlp.forward(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states

class Qwen3ModelWithKVCache:
    """å¸¦KV Cacheçš„Qwen3æ¨¡å‹å®ç°"""
    def __init__(self, weight_manager, config):
        print("æ„å»ºå¸¦KV Cacheçš„Qwen3æ¨¡å‹...")
        self.config = config

        # åµŒå…¥å±‚
        self.embed_tokens = CustomEmbedding(
            weight_manager.get_weight("model.embed_tokens.weight")
        )

        # Transformerå±‚
        self.layers = []
        for i in range(config['num_hidden_layers']):
            self.layers.append(Qwen3DecoderLayerWithKVCache(i, weight_manager, config))

        # æœ€ç»ˆnorm
        self.norm = CustomRMSNorm(
            weight_manager.get_weight("model.norm.weight"),
            eps=config['rms_norm_eps']
        )

        # è¾“å‡ºå±‚
        self.lm_head = CustomLinear(weight_manager.get_weight("lm_head.weight"))

        print(f"âœ… å¸¦KV Cacheçš„Qwen3æ¨¡å‹æ„å»ºå®Œæˆ: {len(self.layers)}å±‚")

    def forward(self, input_ids, attention_mask=None, position_ids=None, use_cache=False, cache_position=None):
        batch_size, seq_len = input_ids.shape

        # åµŒå…¥
        hidden_states = self.embed_tokens.forward(input_ids)

        # å› æœæ©ç 
        if attention_mask is None:
            causal_mask = torch.triu(
                torch.full((seq_len, seq_len), float('-inf')),
                diagonal=1
            )
            attention_mask = causal_mask.unsqueeze(0).unsqueeze(0)

        # é€šè¿‡æ‰€æœ‰å±‚
        for layer in self.layers:
            hidden_states = layer.forward(
                hidden_states, attention_mask, position_ids, use_cache, cache_position
            )

        # æœ€ç»ˆnorm
        hidden_states = self.norm.forward(hidden_states)

        # è¾“å‡º
        logits = self.lm_head.forward(hidden_states)
        return logits

    def reset_kv_cache(self):
        """é‡ç½®æ‰€æœ‰å±‚çš„KVç¼“å­˜"""
        for layer in self.layers:
            layer.self_attn.kv_cache.reset()

    def generate_with_kvcache(self, input_ids, tokenizer, max_new_tokens=50):
        """ä½¿ç”¨KV Cacheçš„ç”Ÿæˆå‡½æ•°"""
        self.reset_kv_cache()

        generated = input_ids.clone()

        # é¦–æ¬¡å‰å‘ä¼ æ’­ï¼ˆå¤„ç†å®Œæ•´çš„promptï¼‰
        with torch.no_grad():
            logits = self.forward(input_ids, use_cache=True)
            next_token_logits = logits[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            generated = torch.cat([generated, next_token], dim=1)

            print(tokenizer.decode(int(next_token), skip_special_tokens=True).strip())

        # å¢é‡ç”Ÿæˆ
        for i in range(max_new_tokens - 1):
            with torch.no_grad():
                # åªä¼ å…¥æ–°çš„token
                logits = self.forward(next_token, use_cache=True)
                next_token_logits = logits[:, -1, :]

                if torch.isnan(next_token_logits).any():
                    print(f"åœ¨æ­¥éª¤{i+1}æ£€æµ‹åˆ°NaN")
                    break

                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                generated = torch.cat([generated, next_token], dim=1)

                print(tokenizer.decode(int(next_token), skip_special_tokens=True).strip(), end="", flush=True)

                if next_token.item() in [151645, 151643]:
                    print()
                    break

        return generated

    def generate(self, input_ids, tokenizer, max_new_tokens=50):
        """æ ‡å‡†ç”Ÿæˆå‡½æ•°ï¼ˆä¸ä½¿ç”¨KV Cacheï¼‰"""
        generated = input_ids.clone()

        for i in range(max_new_tokens):
            logits = self.forward(generated)
            next_token_logits = logits[:, -1, :]

            if torch.isnan(next_token_logits).any():
                print(f"åœ¨æ­¥éª¤{i}æ£€æµ‹åˆ°NaN")
                break

            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            generated = torch.cat([generated, next_token], dim=1)

            print(tokenizer.decode(int(next_token), skip_special_tokens=True).strip(), end="", flush=True)

            if next_token.item() in [151645, 151643]:
                break

        return generated

def test_comparison():
    """å¯¹æ¯”æµ‹è¯•æ ‡å‡†ç‰ˆæœ¬å’ŒKV Cacheç‰ˆæœ¬"""
    print("ğŸ”§ Qwen3æ ‡å‡†ç‰ˆæœ¬ vs KV Cacheç‰ˆæœ¬å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)

    model_path = "models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca"

    # åŠ è½½é…ç½®
    with open(f"{model_path}/config.json", 'r') as f:
        config = json.load(f)

    print(f"æ¨¡å‹é…ç½®:")
    print(f"- æ³¨æ„åŠ›å¤´æ•°: {config['num_attention_heads']}")
    print(f"- KVå¤´æ•°: {config['num_key_value_heads']}")
    print(f"- å¤´ç»´åº¦: {config['head_dim']}")
    print(f"- éšè—ç»´åº¦: {config['hidden_size']}")

    # åˆ›å»ºæ¨¡å‹
    weight_manager = WeightManager(f"{model_path}/model.safetensors")
    model = Qwen3ModelWithKVCache(weight_manager, config)

    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # å‡†å¤‡è¾“å…¥
    prompt = "ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True
    )
    input_ids = tokenizer.encode(text, return_tensors="pt")

    print(f"\nè¾“å…¥prompt: {prompt}")
    print(f"è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")

    # æµ‹è¯•æ ‡å‡†ç‰ˆæœ¬
    print("\n" + "="*30)
    print("ğŸ”„ æ ‡å‡†ç‰ˆæœ¬ç”Ÿæˆ (ä¸ä½¿ç”¨KV Cache):")
    print("="*30)

    import time
    start_time = time.time()
    with torch.no_grad():
        standard_outputs = model.generate(input_ids, tokenizer, max_new_tokens=30)
    standard_time = time.time() - start_time

    # æµ‹è¯•KV Cacheç‰ˆæœ¬
    print("\n" + "="*30)
    print("ğŸš€ KV Cacheç‰ˆæœ¬ç”Ÿæˆ:")
    print("="*30)

    start_time = time.time()
    with torch.no_grad():
        kv_cache_outputs = model.generate_with_kvcache(input_ids, tokenizer, max_new_tokens=30)
    kv_cache_time = time.time() - start_time

    # æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"æ ‡å‡†ç‰ˆæœ¬è€—æ—¶: {standard_time:.2f}ç§’")
    print(f"KV Cacheè€—æ—¶: {kv_cache_time:.2f}ç§’")
    print(f"åŠ é€Ÿæ¯”: {standard_time/kv_cache_time:.2f}x")

    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸€è‡´
    standard_tokens = standard_outputs[0][len(input_ids[0]):].tolist()[:30]
    kv_cache_tokens = kv_cache_outputs[0][len(input_ids[0]):].tolist()[:30]

    print(f"\nğŸ” è¾“å‡ºä¸€è‡´æ€§æ£€æŸ¥:")
    print(f"ç”Ÿæˆtokenæ•°é‡ - æ ‡å‡†ç‰ˆæœ¬: {len(standard_tokens)}, KV Cache: {len(kv_cache_tokens)}")

    min_len = min(len(standard_tokens), len(kv_cache_tokens))
    matches = sum(1 for i in range(min_len) if standard_tokens[i] == kv_cache_tokens[i])

    print(f"å‰{min_len}ä¸ªtokenåŒ¹é…: {matches}/{min_len} ({matches/min_len*100:.1f}%)")

    if matches == min_len:
        print("âœ… è¾“å‡ºå®Œå…¨ä¸€è‡´!")
    else:
        print("âš ï¸ è¾“å‡ºå­˜åœ¨å·®å¼‚")
        print(f"æ ‡å‡†ç‰ˆæœ¬: {tokenizer.decode(standard_tokens[:10])}")
        print(f"KV Cache: {tokenizer.decode(kv_cache_tokens[:10])}")

def main():
    test_comparison()

if __name__ == "__main__":
    main()