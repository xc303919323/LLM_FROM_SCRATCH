import torch
import json
from safetensors import safe_open
from transformers import AutoTokenizer
from qwen3_with_kvcache import *
from correct_qwen3_inference import CustomQwen3Attention

def debug_attention_layer():
    print("ğŸ” è°ƒè¯•å•ä¸ªæ³¨æ„åŠ›å±‚")
    print("=" * 40)

    model_path = "models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca"

    # åŠ è½½é…ç½®
    with open(f"{model_path}/config.json", 'r') as f:
        config = json.load(f)

    # åˆ›å»ºæƒé‡ç®¡ç†å™¨
    weight_manager = WeightManager(f"{model_path}/model.safetensors")

    # åˆ›å»ºåŸå§‹æ³¨æ„åŠ›å±‚å’ŒKV Cacheæ³¨æ„åŠ›å±‚
    original_attn = CustomQwen3Attention(0, weight_manager, config)
    kvcache_attn = Qwen3AttentionWithKVCache(0, weight_manager, config)

    # å‡†å¤‡æµ‹è¯•è¾“å…¥
    batch_size, seq_len, hidden_size = 1, 12, config['hidden_size']
    hidden_states = torch.randn(batch_size, seq_len, hidden_size)

    print(f"è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")

    # åˆ›å»ºå› æœæ©ç 
    causal_mask = torch.triu(
        torch.full((seq_len, seq_len), float('-inf')),
        diagonal=1
    )
    attention_mask = causal_mask.unsqueeze(0).unsqueeze(0)

    # æµ‹è¯•åŸå§‹æ³¨æ„åŠ›å±‚
    with torch.no_grad():
        output1 = original_attn.forward(hidden_states, attention_mask)
        print(f"åŸå§‹æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {output1.shape}")

    # æµ‹è¯•KV Cacheæ³¨æ„åŠ›å±‚ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
    kvcache_attn.kv_cache.reset()
    with torch.no_grad():
        output2 = kvcache_attn.forward(hidden_states, attention_mask, use_cache=False)
        print(f"KV Cacheæ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶ï¼ˆæ— ç¼“å­˜ï¼‰: {output2.shape}")

    # æ¯”è¾ƒè¾“å‡º
    diff = torch.abs(output1 - output2).max()
    print(f"è¾“å‡ºæœ€å¤§å·®å¼‚ï¼ˆæ— ç¼“å­˜ï¼‰: {diff.item()}")

    if diff < 1e-5:
        print("âœ… æ— ç¼“å­˜æ¨¡å¼ä¸€è‡´!")
    else:
        print("âŒ æ— ç¼“å­˜æ¨¡å¼ä¸ä¸€è‡´!")
        return

    # æµ‹è¯•KV Cacheæ³¨æ„åŠ›å±‚ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    kvcache_attn.kv_cache.reset()
    with torch.no_grad():
        output3 = kvcache_attn.forward(hidden_states, attention_mask, use_cache=True)
        print(f"KV Cacheæ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶ï¼ˆæœ‰ç¼“å­˜ï¼‰: {output3.shape}")

    # æ¯”è¾ƒç¼“å­˜å’Œæ— ç¼“å­˜çš„è¾“å‡º
    diff2 = torch.abs(output1 - output3).max()
    print(f"è¾“å‡ºæœ€å¤§å·®å¼‚ï¼ˆæœ‰ç¼“å­˜ vs åŸå§‹ï¼‰: {diff2.item()}")

    if diff2 < 1e-5:
        print("âœ… æœ‰ç¼“å­˜æ¨¡å¼ä¸€è‡´!")
    else:
        print("âŒ æœ‰ç¼“å­˜æ¨¡å¼ä¸ä¸€è‡´!")

    # æµ‹è¯•å¢é‡æ¨ç†
    print("\n=== å¢é‡æ¨ç†æµ‹è¯• ===")

    # å•ä¸ªtokenè¾“å…¥
    single_token = hidden_states[:, :1, :]  # åªå–ç¬¬ä¸€ä¸ªtoken

    with torch.no_grad():
        output4 = kvcache_attn.forward(single_token, use_cache=True)
        print(f"å¢é‡æ¨ç†è¾“å‡ºå½¢çŠ¶: {output4.shape}")

    print("å¢é‡æ¨ç†æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    debug_attention_layer()