import torch
import json
from safetensors import safe_open
from transformers import AutoTokenizer
from qwen3_with_kvcache import *
from correct_qwen3_inference import CorrectQwen3Model

def debug_layer_by_layer():
    print("ğŸ” é€å±‚è°ƒè¯•KV Cacheå®ç°")
    print("=" * 50)

    model_path = "models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca"

    # åŠ è½½é…ç½®
    with open(f"{model_path}/config.json", 'r') as f:
        config = json.load(f)

    # åˆ›å»ºæƒé‡ç®¡ç†å™¨
    weight_manager = WeightManager(f"{model_path}/model.safetensors")

    # åˆ›å»ºåŸå§‹æ¨¡å‹å’ŒKV Cacheæ¨¡å‹
    original_model = CorrectQwen3Model(weight_manager, config)
    kvcache_model = Qwen3ModelWithKVCache(weight_manager, config)

    # æµ‹è¯•è¾“å…¥
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    prompt = "ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True
    )
    input_ids = tokenizer.encode(text, return_tensors="pt")

    print(f"è¾“å…¥token ids: {input_ids}")

    # åˆå§‹åŒ–
    batch_size, seq_len = input_ids.shape

    # åµŒå…¥
    hidden_states_orig = original_model.embed_tokens.forward(input_ids)
    hidden_states_kv = kvcache_model.embed_tokens.forward(input_ids)

    # ä½ç½®ids
    position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device).unsqueeze(0)

    # å› æœæ©ç 
    causal_mask = torch.triu(
        torch.full((seq_len, seq_len), float('-inf')),
        diagonal=1
    )
    attention_mask = causal_mask.unsqueeze(0).unsqueeze(0)

    # é‡ç½®KVç¼“å­˜
    kvcache_model.reset_kv_cache()

    # é€å±‚æ¯”è¾ƒ
    for layer_idx in range(min(3, len(original_model.layers))):  # åªæµ‹è¯•å‰3å±‚
        print(f"\n=== ç¬¬{layer_idx}å±‚ ===")

        # åŸå§‹æ¨¡å‹
        hidden_states_orig = original_model.layers[layer_idx].forward(
            hidden_states_orig, attention_mask, position_ids
        )

        # KV Cacheæ¨¡å‹
        hidden_states_kv = kvcache_model.layers[layer_idx].forward(
            hidden_states_kv, attention_mask, position_ids, use_cache=True
        )

        # æ¯”è¾ƒ
        diff = torch.abs(hidden_states_orig - hidden_states_kv).max()
        print(f"ç¬¬{layer_idx}å±‚è¾“å‡ºå·®å¼‚: {diff.item()}")

        if diff > 1e-5:
            print(f"âŒ ç¬¬{layer_idx}å±‚å‡ºç°å·®å¼‚!")
            break
        else:
            print(f"âœ… ç¬¬{layer_idx}å±‚ä¸€è‡´")

    # å¦‚æœå‰é¢çš„å±‚éƒ½ä¸€è‡´ï¼Œç»§ç»­æµ‹è¯•å‰©ä½™å±‚
    if diff <= 1e-5:
        print(f"\nç»§ç»­æµ‹è¯•å‰©ä½™å±‚...")
        for layer_idx in range(3, len(original_model.layers)):
            # åŸå§‹æ¨¡å‹
            hidden_states_orig = original_model.layers[layer_idx].forward(
                hidden_states_orig, attention_mask, position_ids
            )

            # KV Cacheæ¨¡å‹
            hidden_states_kv = kvcache_model.layers[layer_idx].forward(
                hidden_states_kv, attention_mask, position_ids, use_cache=True
            )

            # æ¯”è¾ƒ
            diff = torch.abs(hidden_states_orig - hidden_states_kv).max()

            if diff > 1e-5:
                print(f"âŒ ç¬¬{layer_idx}å±‚å‡ºç°å·®å¼‚: {diff.item()}")
                break
            elif layer_idx % 5 == 0:  # æ¯5å±‚æŠ¥å‘Šä¸€æ¬¡
                print(f"âœ… ç¬¬{layer_idx}å±‚ä¸€è‡´: {diff.item()}")

    print(f"\né—®é¢˜å‡ºç°åœ¨ç¬¬{layer_idx}å±‚")

if __name__ == "__main__":
    debug_layer_by_layer()