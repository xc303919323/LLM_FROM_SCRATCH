import torch
import json
from safetensors import safe_open
from transformers import AutoTokenizer
from qwen3_with_kvcache import *
from correct_qwen3_inference import CorrectQwen3Model

def debug_final_layers():
    print("ğŸ” è°ƒè¯•æœ€ç»ˆå±‚ï¼ˆnormå’Œlm_headï¼‰")
    print("=" * 50)

    model_path = "models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca"

    # åŠ è½½é…ç½®
    with open(f"{model_path}/config.json", 'r') as f:
        config = json.load(f)

    # åˆ›å»ºæƒé‡ç®¡ç†å™¨
    weight_manager = WeightManager(f"{model_path}/model.safetensors")

    # åˆ›å»ºåŸå§‹æ¨¡å‹å’ŒKV Cacheæ¨¡å‹
    original_model = CorrectQwen3Model(weight_manager, config)
    kvcache_model = Qwen3ModelWithKVCache(weight_manager, config)

    # æµ‹è¯•è¾“å…¥
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    prompt = "ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True
    )
    input_ids = tokenizer.encode(text, return_tensors="pt")

    print(f"è¾“å…¥token ids: {input_ids}")

    # å®Œæ•´å‰å‘ä¼ æ’­åˆ°æœ€åä¸€å±‚ä¹‹å‰
    with torch.no_grad():
        # åŸå§‹æ¨¡å‹
        batch_size, seq_len = input_ids.shape

        # åµŒå…¥
        hidden_states_orig = original_model.embed_tokens.forward(input_ids)

        # ä½ç½®ids
        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device).unsqueeze(0)

        # å› æœæ©ç 
        causal_mask = torch.triu(
            torch.full((seq_len, seq_len), float('-inf')),
            diagonal=1
        )
        attention_mask = causal_mask.unsqueeze(0).unsqueeze(0)

        # é€šè¿‡æ‰€æœ‰å±‚
        for layer in original_model.layers:
            hidden_states_orig = layer.forward(hidden_states_orig, attention_mask, position_ids)

        # KV Cacheæ¨¡å‹
        kvcache_model.reset_kv_cache()
        hidden_states_kv = kvcache_model.embed_tokens.forward(input_ids)

        # é€šè¿‡æ‰€æœ‰å±‚
        for layer in kvcache_model.layers:
            hidden_states_kv = layer.forward(
                hidden_states_kv, attention_mask, position_ids, use_cache=True
            )

        # æ¯”è¾ƒæ‰€æœ‰å±‚åçš„hidden states
        diff_before_norm = torch.abs(hidden_states_orig - hidden_states_kv).max()
        print(f"æ‰€æœ‰å±‚åçš„hidden stateså·®å¼‚: {diff_before_norm.item()}")

        if diff_before_norm > 1e-5:
            print("âŒ æ‰€æœ‰å±‚åå°±å·²ç»ä¸ä¸€è‡´!")
            return

        # åº”ç”¨æœ€ç»ˆnorm
        normed_orig = original_model.norm.forward(hidden_states_orig)
        normed_kv = kvcache_model.norm.forward(hidden_states_kv)

        diff_after_norm = torch.abs(normed_orig - normed_kv).max()
        print(f"Normåçš„å·®å¼‚: {diff_after_norm.item()}")

        # æ¯”è¾ƒnormæƒé‡
        norm_weight_diff = torch.abs(
            original_model.norm.weight - kvcache_model.norm.weight
        ).max()
        print(f"Normæƒé‡å·®å¼‚: {norm_weight_diff.item()}")

        # åº”ç”¨lm_head
        logits_orig = original_model.lm_head.forward(normed_orig)
        logits_kv = kvcache_model.lm_head.forward(normed_kv)

        diff_logits = torch.abs(logits_orig - logits_kv).max()
        print(f"Logitså·®å¼‚: {diff_logits.item()}")

        # æ¯”è¾ƒlm_headæƒé‡
        lm_head_weight_diff = torch.abs(
            original_model.lm_head.weight - kvcache_model.lm_head.weight
        ).max()
        print(f"LM headæƒé‡å·®å¼‚: {lm_head_weight_diff.item()}")

        # æ£€æŸ¥å…·ä½“çš„logitså€¼
        print(f"\nåŸå§‹æ¨¡å‹æœ€åä½ç½®å‰10ä¸ªlogits: {logits_orig[0, -1, :10]}")
        print(f"KV Cacheæ¨¡å‹æœ€åä½ç½®å‰10ä¸ªlogits: {logits_kv[0, -1, :10]}")

        # åˆ†åˆ«ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œæ­£å¸¸æ¨ç†å’Œä½¿ç”¨ç¼“å­˜æ¨ç†
        print(f"\n=== å¯¹æ¯”åŸå§‹æ¨¡å‹çš„ç¼“å­˜å’Œéç¼“å­˜ç‰ˆæœ¬ ===")

        # åŸå§‹æ¨¡å‹æ­£å¸¸æ¨ç†
        logits_orig_normal = original_model.forward(input_ids)

        # æ‰‹åŠ¨è°ƒç”¨KV Cacheæ¨¡å‹çš„æ­£å¸¸æ¨ç†
        logits_kv_normal = kvcache_model.forward(input_ids, use_cache=False)

        # KV Cacheæ¨¡å‹ç¼“å­˜æ¨ç†
        kvcache_model.reset_kv_cache()
        logits_kv_cached = kvcache_model.forward(input_ids, use_cache=True)

        print(f"åŸå§‹æ¨¡å‹æ­£å¸¸æ¨ç†: {torch.argmax(logits_orig_normal[0, -1, :]).item()}")
        print(f"KVæ¨¡å‹æ­£å¸¸æ¨ç†: {torch.argmax(logits_kv_normal[0, -1, :]).item()}")
        print(f"KVæ¨¡å‹ç¼“å­˜æ¨ç†: {torch.argmax(logits_kv_cached[0, -1, :]).item()}")

        diff_normal = torch.abs(logits_orig_normal - logits_kv_normal).max()
        diff_cached = torch.abs(logits_orig_normal - logits_kv_cached).max()

        print(f"æ­£å¸¸æ¨ç†å·®å¼‚: {diff_normal.item()}")
        print(f"ç¼“å­˜æ¨ç†å·®å¼‚: {diff_cached.item()}")

if __name__ == "__main__":
    debug_final_layers()