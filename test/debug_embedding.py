import torch
import json
from safetensors import safe_open
from transformers import AutoTokenizer
from qwen3_with_kvcache import *
from correct_qwen3_inference import CorrectQwen3Model

def debug_embedding_and_early_layers():
    print("ğŸ” è°ƒè¯•åµŒå…¥å±‚å’Œæ—©æœŸå±‚")
    print("=" * 40)

    model_path = "models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca"

    # åŠ è½½é…ç½®
    with open(f"{model_path}/config.json", 'r') as f:
        config = json.load(f)

    # åˆ›å»ºæƒé‡ç®¡ç†å™¨
    weight_manager = WeightManager(f"{model_path}/model.safetensors")

    # åˆ›å»ºåŸå§‹æ¨¡å‹å’ŒKV Cacheæ¨¡å‹
    original_model = CorrectQwen3Model(weight_manager, config)
    kvcache_model = Qwen3ModelWithKVCache(weight_manager, config)

    # æµ‹è¯•è¾“å…¥
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    prompt = "ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True
    )
    input_ids = tokenizer.encode(text, return_tensors="pt")

    print(f"è¾“å…¥token ids: {input_ids}")

    # æ¯”è¾ƒåµŒå…¥å±‚è¾“å‡º
    embed1 = original_model.embed_tokens.forward(input_ids)
    embed2 = kvcache_model.embed_tokens.forward(input_ids)

    embed_diff = torch.abs(embed1 - embed2).max()
    print(f"åµŒå…¥å±‚å·®å¼‚: {embed_diff.item()}")

    if embed_diff < 1e-10:
        print("âœ… åµŒå…¥å±‚ä¸€è‡´!")
    else:
        print("âŒ åµŒå…¥å±‚ä¸ä¸€è‡´!")
        return

    # æµ‹è¯•ç¬¬ä¸€å±‚çš„è¾“å‡º
    print("\n=== ç¬¬ä¸€å±‚å¯¹æ¯” ===")

    # å› æœæ©ç 
    seq_len = input_ids.shape[1]
    causal_mask = torch.triu(
        torch.full((seq_len, seq_len), float('-inf')),
        diagonal=1
    )
    attention_mask = causal_mask.unsqueeze(0).unsqueeze(0)

    # ä½ç½®ids
    position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device).unsqueeze(0)

    # åŸå§‹æ¨¡å‹ç¬¬ä¸€å±‚
    layer1_input = embed1
    layer1_output_orig = original_model.layers[0].forward(layer1_input, attention_mask, position_ids)

    # KV Cacheæ¨¡å‹ç¬¬ä¸€å±‚ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
    kvcache_model.reset_kv_cache()
    layer1_output_kv_nocache = kvcache_model.layers[0].forward(layer1_input, attention_mask, position_ids, use_cache=False)

    layer1_diff = torch.abs(layer1_output_orig - layer1_output_kv_nocache).max()
    print(f"ç¬¬ä¸€å±‚è¾“å‡ºå·®å¼‚ï¼ˆæ— ç¼“å­˜ï¼‰: {layer1_diff.item()}")

    if layer1_diff < 1e-5:
        print("âœ… ç¬¬ä¸€å±‚æ— ç¼“å­˜ä¸€è‡´!")
    else:
        print("âŒ ç¬¬ä¸€å±‚æ— ç¼“å­˜ä¸ä¸€è‡´!")

    # KV Cacheæ¨¡å‹ç¬¬ä¸€å±‚ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    kvcache_model.reset_kv_cache()
    layer1_output_kv_cache = kvcache_model.layers[0].forward(layer1_input, attention_mask, position_ids, use_cache=True)

    layer1_diff_cache = torch.abs(layer1_output_orig - layer1_output_kv_cache).max()
    print(f"ç¬¬ä¸€å±‚è¾“å‡ºå·®å¼‚ï¼ˆæœ‰ç¼“å­˜ï¼‰: {layer1_diff_cache.item()}")

    if layer1_diff_cache < 1e-5:
        print("âœ… ç¬¬ä¸€å±‚æœ‰ç¼“å­˜ä¸€è‡´!")
    else:
        print("âŒ ç¬¬ä¸€å±‚æœ‰ç¼“å­˜ä¸ä¸€è‡´!")

    # æµ‹è¯•å®Œæ•´æ¨¡å‹å‰å‘ä¼ æ’­
    print("\n=== å®Œæ•´æ¨¡å‹å‰å‘ä¼ æ’­å¯¹æ¯” ===")

    # åŸå§‹æ¨¡å‹
    with torch.no_grad():
        logits_orig = original_model.forward(input_ids)
        next_token_orig = torch.argmax(logits_orig[:, -1, :], dim=-1, keepdim=True)

    # KV Cacheæ¨¡å‹ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
    kvcache_model.reset_kv_cache()
    with torch.no_grad():
        logits_kv_nocache = kvcache_model.forward(input_ids, use_cache=False)
        next_token_kv_nocache = torch.argmax(logits_kv_nocache[:, -1, :], dim=-1, keepdim=True)

    # KV Cacheæ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    kvcache_model.reset_kv_cache()
    with torch.no_grad():
        logits_kv_cache = kvcache_model.forward(input_ids, use_cache=True)
        next_token_kv_cache = torch.argmax(logits_kv_cache[:, -1, :], dim=-1, keepdim=True)

    print(f"åŸå§‹æ¨¡å‹ä¸‹ä¸€ä¸ªtoken: {next_token_orig.item()} -> '{tokenizer.decode(next_token_orig.item())}'")
    print(f"KVæ¨¡å‹æ— ç¼“å­˜ä¸‹ä¸€ä¸ªtoken: {next_token_kv_nocache.item()} -> '{tokenizer.decode(next_token_kv_nocache.item())}'")
    print(f"KVæ¨¡å‹æœ‰ç¼“å­˜ä¸‹ä¸€ä¸ªtoken: {next_token_kv_cache.item()} -> '{tokenizer.decode(next_token_kv_cache.item())}'")

    # æ¯”è¾ƒlogits
    logits_diff_nocache = torch.abs(logits_orig[:, -1, :] - logits_kv_nocache[:, -1, :]).max()
    logits_diff_cache = torch.abs(logits_orig[:, -1, :] - logits_kv_cache[:, -1, :]).max()

    print(f"Logitså·®å¼‚ï¼ˆæ— ç¼“å­˜ï¼‰: {logits_diff_nocache.item()}")
    print(f"Logitså·®å¼‚ï¼ˆæœ‰ç¼“å­˜ï¼‰: {logits_diff_cache.item()}")

if __name__ == "__main__":
    debug_embedding_and_early_layers()