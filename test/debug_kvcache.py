import torch
import json
from safetensors import safe_open
from transformers import AutoTokenizer
from qwen3_with_kvcache import *

def debug_kv_cache():
    print("ğŸ” è°ƒè¯•KV Cacheå®ç°")
    print("=" * 40)

    model_path = "models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca"

    # åŠ è½½é…ç½®
    with open(f"{model_path}/config.json", 'r') as f:
        config = json.load(f)

    # åˆ›å»ºæ¨¡å‹
    weight_manager = WeightManager(f"{model_path}/model.safetensors")
    model = Qwen3ModelWithKVCache(weight_manager, config)

    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # ç®€å•æµ‹è¯•è¾“å…¥
    test_text = "Hello"
    input_ids = tokenizer.encode(test_text, return_tensors="pt")
    print(f"è¾“å…¥: {test_text}")
    print(f"Token IDs: {input_ids}")

    # æµ‹è¯•æ ‡å‡†å‰å‘ä¼ æ’­
    print("\næ ‡å‡†å‰å‘ä¼ æ’­:")
    with torch.no_grad():
        logits1 = model.forward(input_ids, use_cache=False)
        next_token1 = torch.argmax(logits1[:, -1, :], dim=-1, keepdim=True)
        print(f"ä¸‹ä¸€ä¸ªtoken: {next_token1.item()}")
        print(f"è§£ç : {tokenizer.decode(next_token1.item())}")

    # æµ‹è¯•KV Cacheå‰å‘ä¼ æ’­
    print("\nKV Cacheå‰å‘ä¼ æ’­:")
    model.reset_kv_cache()
    with torch.no_grad():
        logits2 = model.forward(input_ids, use_cache=True)
        next_token2 = torch.argmax(logits2[:, -1, :], dim=-1, keepdim=True)
        print(f"ä¸‹ä¸€ä¸ªtoken: {next_token2.item()}")
        print(f"è§£ç : {tokenizer.decode(next_token2.item())}")

    # æ£€æŸ¥logitsæ˜¯å¦ä¸€è‡´
    print(f"\nLogitsä¸€è‡´æ€§:")
    print(f"Logitså½¢çŠ¶ - æ ‡å‡†: {logits1.shape}, KV Cache: {logits2.shape}")

    # æ¯”è¾ƒæœ€åä¸€ä¸ªä½ç½®çš„logits
    diff = torch.abs(logits1[:, -1, :] - logits2[:, -1, :]).max()
    print(f"æœ€å¤§å·®å¼‚: {diff.item()}")

    if diff < 1e-5:
        print("âœ… Logitsä¸€è‡´!")
    else:
        print("âŒ Logitsä¸ä¸€è‡´!")
        # æ˜¾ç¤ºå‰10ä¸ªlogitså€¼
        print("æ ‡å‡†ç‰ˆæœ¬å‰10ä¸ªlogits:", logits1[0, -1, :10])
        print("KV Cacheå‰10ä¸ªlogits:", logits2[0, -1, :10])

    # æµ‹è¯•å¢é‡æ¨ç†
    print(f"\nå¢é‡æ¨ç†æµ‹è¯•:")
    with torch.no_grad():
        # ä½¿ç”¨KV Cacheçš„ä¸‹ä¸€ä¸ªtoken
        logits3 = model.forward(next_token2, use_cache=True)
        next_token3 = torch.argmax(logits3[:, -1, :], dim=-1, keepdim=True)
        print(f"å¢é‡æ¨ç†ä¸‹ä¸€ä¸ªtoken: {next_token3.item()}")
        print(f"è§£ç : {tokenizer.decode(next_token3.item())}")

    # æ¯”è¾ƒä¸å®Œæ•´åºåˆ—çš„ç»“æœ
    full_sequence = torch.cat([input_ids, next_token2], dim=1)
    print(f"\nå®Œæ•´åºåˆ—å¯¹æ¯”:")
    with torch.no_grad():
        logits_full = model.forward(full_sequence, use_cache=False)
        next_token_full = torch.argmax(logits_full[:, -1, :], dim=-1, keepdim=True)
        print(f"å®Œæ•´åºåˆ—ä¸‹ä¸€ä¸ªtoken: {next_token_full.item()}")

    if next_token3.item() == next_token_full.item():
        print("âœ… å¢é‡æ¨ç†æ­£ç¡®!")
    else:
        print("âŒ å¢é‡æ¨ç†é”™è¯¯!")

    # æµ‹è¯•å®Œæ•´çš„ç”Ÿæˆè¿‡ç¨‹å¯¹æ¯”
    print(f"\nå®Œæ•´ç”Ÿæˆè¿‡ç¨‹å¯¹æ¯”:")

    # æ ‡å‡†ç”Ÿæˆ
    model.reset_kv_cache()  # é‡ç½®ç¼“å­˜
    with torch.no_grad():
        standard_result = model.generate(input_ids, tokenizer, max_new_tokens=10)
    print("æ ‡å‡†ç”Ÿæˆç»“æœ:", tokenizer.decode(standard_result[0].tolist()))

    # KV Cacheç”Ÿæˆ
    model.reset_kv_cache()  # é‡ç½®ç¼“å­˜
    with torch.no_grad():
        kvcache_result = model.generate_with_kvcache(input_ids, tokenizer, max_new_tokens=10)
    print("KV Cacheç”Ÿæˆç»“æœ:", tokenizer.decode(kvcache_result[0].tolist()))

if __name__ == "__main__":
    debug_kv_cache()