import torch
import json
from safetensors import safe_open
from transformers import AutoTokenizer
from qwen3_with_kvcache import *

def debug_complex_input():
    print("ğŸ” è°ƒè¯•å¤æ‚è¾“å…¥çš„KV Cacheå®ç°")
    print("=" * 50)

    model_path = "models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca"

    # åŠ è½½é…ç½®
    with open(f"{model_path}/config.json", 'r') as f:
        config = json.load(f)

    # åˆ›å»ºæ¨¡å‹
    weight_manager = WeightManager(f"{model_path}/model.safetensors")
    model = Qwen3ModelWithKVCache(weight_manager, config)

    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # å‡†å¤‡å¤æ‚è¾“å…¥
    prompt = "ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True
    )
    input_ids = tokenizer.encode(text, return_tensors="pt")

    print(f"Chat templateæ–‡æœ¬:\n{text}")
    print(f"\nToken IDs: {input_ids}")
    print(f"è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")
    print(f"è§£ç å›æ¥: {tokenizer.decode(input_ids[0].tolist())}")

    # æµ‹è¯•ç¬¬ä¸€æ­¥
    print("\n=== ç¬¬ä¸€æ­¥å¯¹æ¯” ===")

    # æ ‡å‡†å‰å‘ä¼ æ’­
    model.reset_kv_cache()
    with torch.no_grad():
        logits1 = model.forward(input_ids, use_cache=False)
        next_token1 = torch.argmax(logits1[:, -1, :], dim=-1, keepdim=True)
        print(f"æ ‡å‡†ç‰ˆæœ¬ä¸‹ä¸€ä¸ªtoken: {next_token1.item()}")
        print(f"è§£ç : '{tokenizer.decode(next_token1.item())}'")

    # KV Cacheå‰å‘ä¼ æ’­
    model.reset_kv_cache()
    with torch.no_grad():
        logits2 = model.forward(input_ids, use_cache=True)
        next_token2 = torch.argmax(logits2[:, -1, :], dim=-1, keepdim=True)
        print(f"KV Cacheç‰ˆæœ¬ä¸‹ä¸€ä¸ªtoken: {next_token2.item()}")
        print(f"è§£ç : '{tokenizer.decode(next_token2.item())}'")

    # æ£€æŸ¥logitså·®å¼‚
    diff = torch.abs(logits1[:, -1, :] - logits2[:, -1, :]).max()
    print(f"Logitsæœ€å¤§å·®å¼‚: {diff.item()}")

    if next_token1.item() == next_token2.item():
        print("âœ… ç¬¬ä¸€æ­¥ä¸€è‡´!")
    else:
        print("âŒ ç¬¬ä¸€æ­¥ä¸ä¸€è‡´!")
        return

    # æµ‹è¯•ç¬¬äºŒæ­¥
    print("\n=== ç¬¬äºŒæ­¥å¯¹æ¯” ===")

    # æ ‡å‡†ç‰ˆæœ¬: å®Œæ•´åºåˆ—
    full_seq1 = torch.cat([input_ids, next_token1], dim=1)
    with torch.no_grad():
        logits1_step2 = model.forward(full_seq1, use_cache=False)
        next_token1_step2 = torch.argmax(logits1_step2[:, -1, :], dim=-1, keepdim=True)
        print(f"æ ‡å‡†ç‰ˆæœ¬ç¬¬äºŒæ­¥token: {next_token1_step2.item()}")
        print(f"è§£ç : '{tokenizer.decode(next_token1_step2.item())}'")

    # KV Cacheç‰ˆæœ¬: å¢é‡æ¨ç†
    with torch.no_grad():
        logits2_step2 = model.forward(next_token2, use_cache=True)
        next_token2_step2 = torch.argmax(logits2_step2[:, -1, :], dim=-1, keepdim=True)
        print(f"KV Cacheç‰ˆæœ¬ç¬¬äºŒæ­¥token: {next_token2_step2.item()}")
        print(f"è§£ç : '{tokenizer.decode(next_token2_step2.item())}'")

    # æ£€æŸ¥ç¬¬äºŒæ­¥logitså·®å¼‚
    diff2 = torch.abs(logits1_step2[:, -1, :] - logits2_step2[:, -1, :]).max()
    print(f"ç¬¬äºŒæ­¥Logitsæœ€å¤§å·®å¼‚: {diff2.item()}")

    if next_token1_step2.item() == next_token2_step2.item():
        print("âœ… ç¬¬äºŒæ­¥ä¸€è‡´!")
    else:
        print("âŒ ç¬¬äºŒæ­¥ä¸ä¸€è‡´!")
        print("å¯èƒ½æ˜¯ä½ç½®ç¼–ç æˆ–ç¼“å­˜é€»è¾‘é—®é¢˜")

if __name__ == "__main__":
    debug_complex_input()